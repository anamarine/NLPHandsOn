---
title: 'NLP Deliverable'
author: 'Ana Marin Esta√±'
output: html_notebook
---

# **Madrid's airbnb reviews, sentiment analysis**

&nbsp;

&nbsp;

## 0. Select mode
To speed up execution, if sampleReviews variable is set to true (default), only a 1% random sample of the reviews is selected to work with. 
```{r}
sampleReviews = TRUE
```

## 1. Check prerequesites

* Check if all needed packages are installed and added to the library
* Set work directory


```{r}
options(warn=-1)

packages <- c('rstudioapi', 
              'DT', 
              'tm',
              'ggplot2',
              'textcat',
              'wordcloud',
              'RWeka',
              'reshape2', 
              'tidyverse', 
              'tidytext', 
              'stringr',
              'dplyr', 
              'topicmodels')

install.packages(setdiff(packages, rownames(installed.packages())))  
rm(packages)

library(rstudioapi)
library(DT)
library(tm)
library(ggplot2)
library(textcat)
library(wordcloud)
library(RWeka)
library(reshape2)
library(tidyverse)
library(tidytext)
library(stringr)
library(dplyr)
library(topicmodels)
```

```{r}
setwd(dirname(getActiveDocumentContext()$path ))
cat('Working directory:', getwd())
```

&nbsp;

## 2. Import data and basic exploration

* Import review data form data folder
* Explore number of reviews and features
* Display basic data table

```{r}
reviews <- read_csv('data/reviews.csv')
cat('Number of reviews:', nrow(reviews), '\nColumn names:', names(reviews))
```

```{r}
datatable(head(reviews, 10))
```

&nbsp;

## 3. Basic data preprocessing 

* Limit data to 1% random reviews sample (if selected) 
* Detect review language
* Group reviews by language and show 10 most frequent languages
* Keep reviews written in English and Spanish (two most frequent languages) in two different data frames
* Remove empty reviews

```{r}
if(sampleReviews) {
  reviewsPrep = reviews[sample(nrow(reviews), as.integer(nrow(reviews)/100)),]
} else {
  reviewsPrep = reviews
}

reviewsPrep$language <- textcat(reviewsPrep$comments)
reviewsPrep$language <- as.factor(reviewsPrep$language)

languageCount = count(reviewsPrep, language)
languageCount = languageCount[order(languageCount$n, decreasing = T),]
languageCount$percentage <- (languageCount$n/nrow(reviewsPrep))*100

reviewsEn = reviewsPrep[c(reviewsPrep$language == 'english'),]
reviewsEs = reviewsPrep[c(reviewsPrep$language == 'spanish'),]

reviewsEn = reviewsEn[!(is.na(reviewsEn$comments) | reviewsEn$comments==''), ]
reviewsEs = reviewsEs[!(is.na(reviewsEs$comments) | reviewsEs$comments==''), ]
```

&nbsp;

## 4. Corpora creation, inspection and transformations 

Two different corpora are created: one containing English reviews, and other with the Spanish ones.
First, these corpora are inspected:

* Check length
* Summary of 5 first entries
* Meta and content of first entry

Each of this corpora is preprocessed:

* Transform to lower case
* Remove numbers, Stopwords and punctuation
* Stemming
* Strip white spaces

Finally, the first document of each corpus is compared before and after being preprocessed

### English
```{r}
# Creation and inspection
corpusEn = VCorpus(VectorSource(reviewsEn$comments))
cat('English corpus length:', length(corpusEn), 'entries')

summary(corpusEn[1:5]); meta(corpusEn[[1]]); content(corpusEn[[1]])

# Transformation
stopwordsEn = c(stopwords(),'airbnb', 'madrid','stay')
corpusEnT <- tm_map(corpusEn, content_transformer(tolower))
corpusEnT <- tm_map(corpusEnT, content_transformer(removeNumbers))
corpusEnT <- tm_map(corpusEnT, content_transformer(removeWords), stopwordsEn)
corpusEnT <- tm_map(corpusEnT, content_transformer(removePunctuation))
corpusEnT <- tm_map(corpusEnT, content_transformer(stemDocument))
corpusEnT <- tm_map(corpusEnT, content_transformer(stripWhitespace))
```

Original and transformed reviews
```{r}
corpusEn[['100']][['content']]
corpusEnT[['100']][['content']]
```

### Spanish
```{r}
# Creation and inspection
corpusEs = VCorpus(VectorSource(reviewsEs$comments))
cat('Spanish corpus length:', length(corpusEs), 'entries')
summary(corpusEs[1:5]); meta(corpusEs[[1]]); content(corpusEs[[1]])

# Transformation
stopwordsEs = c(stopwords('spanish'), 'madrid', 'airbnb')
corpusEsT <- tm_map(corpusEs, content_transformer(tolower))
corpusEsT <- tm_map(corpusEsT, content_transformer(removeNumbers))
corpusEsT <- tm_map(corpusEsT, content_transformer(removeWords), stopwordsEs)
corpusEsT <- tm_map(corpusEsT, content_transformer(removePunctuation))
corpusEsT <- tm_map(corpusEsT, content_transformer(stemDocument))
corpusEsT <- tm_map(corpusEsT, content_transformer(stripWhitespace))
```

Original and transformed reviews
```{r}
corpusEs[['50']][['content']]
corpusEsT[['50']][['content']]
```

&nbsp;

## 5. Create TDM

In this section Term-Document matrixes are created with TF-IDF weighting

```{r}
tdmEn = TermDocumentMatrix(corpusEnT, control = list(weighting = weightTfIdf))
tdmEn

tdmEs = TermDocumentMatrix(corpusEsT, control = list(weighting = weightTfIdf))
tdmEs
```

&nbsp;

## 6. Basic text analysis

* Language analysis
* TF-IDF word frequencies (Zipf's law comporbation)
* Word clouds
* Most frequent word associations
```{r}
# Adpaat params to number of reviews
if(sampleReviews) {
  maxWords = 100
  lowFreqEn = 1000
  lowFreqEs = 500
} else {
  maxWords = 10000
  lowFreqEn = 100000
  lowFreqEs = 50000
}
```

```{r}
# Language analysis
languageCount[1:10,]
p <- ggplot(data=select(languageCount[1:10, ], language, n), aes(x=reorder(language, -n), y=n)) +
  geom_bar(stat='identity', fill='#2F3456')
p + xlab('Reviews') +
  ylab('Detected language') +
  coord_flip()
```

```{r}
#TF-IDF Zipf's law
freqEn=rowSums(as.matrix(tdmEn))
freqEs=rowSums(as.matrix(tdmEs))
par(mfrow=c(2,1), mar = c(2, 2, 2, 2))
plot(sort(freqEn, decreasing = TRUE), col='#2F3456', xlab='TF-IDF-based rank', ylab = 'TF-IDF', main='Word TF-IDF frequencies (English)')
plot(sort(freqEs, decreasing = TRUE), col='#E77161', xlab='TF-IDF-based rank', ylab = 'TF-IDF', main='Word TF-IDF frequencies (Spanish)')
print('10 most relevant words in English:'); tail(sort(freqEn),n=10)
print('10 most relevant words in Spanish:'); tail(sort(freqEs),n=10)
```

```{r}
#Word Clouds
par(mfrow=c(1,1))

unigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
tdmEn.unigram = TermDocumentMatrix(corpusEnT,
                                control = list (weighting = weightTfIdf,
                                                tokenize = unigramTokenizer))
tdmEs.unigram = TermDocumentMatrix(corpusEsT,
                                   control = list (weighting = weightTfIdf,
                                                   tokenize = unigramTokenizer))
```

```{r}
# Plot English Wordcloud
freqEn = sort(rowSums(as.matrix(tdmEn.unigram)),decreasing = TRUE)
freqEn.df = data.frame(word=names(freqEn), freq=freqEn)
layout(matrix(c(1, 2), nrow=2), heights=c(1, 10))
par(mar=rep(0, 4))
plot.new()
text(x=0.5, y=0.5, "English wordcloud", cex=1.5,font=2)
wordcloud(freqEn.df$word,freqEn.df$freq,max.words=maxWords,random.order = FALSE, colors=brewer.pal(6,'Paired'))

```

```{r}
# Plot Spanish Wordcloud
freqEs = sort(rowSums(as.matrix(tdmEs.unigram)),decreasing = TRUE)
freqEs.df = data.frame(word=names(freqEs), freq=freqEs)
layout(matrix(c(1, 2), nrow=2), heights=c(1, 10))
par(mar=rep(0, 4))
plot.new()
text(x=0.5, y=0.5, "Spanish wordcloud", cex=1.5,font=2)
wordcloud(freqEs.df$word,freqEs.df$freq,max.words=maxWords,random.order = FALSE, colors=brewer.pal(6,'Paired'))
```

```{r}
#Word frequencies and associations
freqTermEn = findFreqTerms(dtmEn, lowfreq = lowFreqEn)
assocEn = findAssocs(dtmEn, terms = freqTermEn, corlimit = 0.15)	
freqTermEn
assocEn

freqTermEs = findFreqTerms(dtmEs, lowfreq = lowFreqEs)
assocEs = findAssocs(dtmEs, terms = freqTermEs, corlimit = 0.15)	
freqTermEs
assocEs
```

&nbsp;

## 7. Sentiment analysis

* General trends
* Most polarized reviews
* Most sentiment-contributing terms

```{r}
# Adpaat params to number of reviews
if(sampleReviews) {
  termFreq = 40
} else {
  termFreq = 1500
}
```

```{r}
#Get sentiments data frame
dtmEn <- DocumentTermMatrix(corpusEnT)
dtmEnTidy <- tidy(dtmEn)
sentimentsEn <- dtmEnTidy %>%
  inner_join(get_sentiments('bing'), by = c(term = 'word'))

positiveWordsEs <- read_csv('es_lexicons/positive_words_es.txt', col_names = "term") %>%
  mutate(sentiment = "positive")
negativeWordsEs <- read_csv('es_lexicons/negative_words_es.txt', col_names = "term") %>%
  mutate(sentiment = "negative")
sentimentWordsEs <- bind_rows(positiveWordsEs, negativeWordsEs)
dtmEs <- DocumentTermMatrix(corpusEsT)
dtmEsTidy <- tidy(dtmEs)
sentimentsEs <- dtmEsTidy %>%
  inner_join(sentimentWordsEs) %>%
  count(document, term, sentiment, sort = TRUE) %>%
  ungroup()
names(sentimentsEs)[4] <- 'count'
```

### English
```{r}
#General trends
sentimentsEn %>%
  count(sentiment, wt = count) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative) %>%
  arrange(-sentiment)
```

```{r}
# Find most positive reviews
posRevs <- sentimentsEn %>%
  count(document, sentiment, wt = count) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative) %>%
  arrange(-sentiment) %>%
  head()
```

```{r}
# Inspect most positive doc
sentimentsEn %>%
  filter(document == posRevs$document[1]) %>%
  arrange(-count)
corpusEn[[posRevs$document[1]]][['content']]  

```

```{r}
# Find most negative reviews
negRevs <- sentimentsEn %>%
  count(document, sentiment, wt = count) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative) %>%
  arrange(sentiment)
```

```{r}
# Inspect most negative doc
sentimentsEn %>%
  filter(document == negRevs$document[1]) %>%
  arrange(-count)
corpusEn[[negRevs$document[1]]][['content']]
```

```{r}
# Most sentiment-contributive words 
sentimentsEn %>%
  count(sentiment, term, wt = count) %>%
  filter(n >= termFreq) %>%
  mutate(n = ifelse(sentiment == 'negative', -n, n)) %>%
  mutate(term = reorder(term, n)) %>%
  ggplot(aes(term, n, fill = sentiment)) +
  geom_bar(stat = 'identity') +
  coord_flip() +
  geom_col() +
  theme() +
  labs(x = 'Term', y = 'Contribution to sentiment',  title = 'English')
```

### Spanish
```{r}
# General trends
sentimentsEs %>%
  count(sentiment, wt = count) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative) %>%
  arrange(-sentiment)
```

```{r}
# Find most positive reviews
posRevs <- sentimentsEs %>%
  count(document, sentiment, wt = count) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative) %>%
  arrange(-sentiment)
```

```{r}
# Inspect most positive doc
sentimentsEs %>%
  filter(document == posRevs$document[1]) %>%
  arrange(-count)
corpusEs[[posRevs$document[1]]][['content']]
```

```{r}
# Find most negative reviews
negRevs <- sentimentsEs %>%
  count(document, sentiment, wt = count) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative) %>%
  arrange(sentiment)
```

```{r}
# Inspect most negative doc
sentimentsEs %>%
  filter(document == negRevs$document[1]) %>%
  arrange(-count)
corpusEs[[negRevs$document[1]]][['content']]
```

```{r}
# Most sentiment-contributive words 
sentimentsEs %>%
  count(sentiment, term, wt = count) %>%
  filter(n >= termFreq) %>%
  mutate(n = ifelse(sentiment == 'negative', -n, n)) %>%
  mutate(term = reorder(term, n)) %>%
  ggplot(aes(term, n, fill = sentiment)) +
  geom_bar(stat = 'identity') +
  coord_flip() +
  geom_col() +
  theme() +
  labs(x = "Term", y = 'Contribution to sentiment',  title = "Spanish")
```






